---
title: "LUME: LLM Unlearning with Multitask Evaluations"
collection: publications
Authors: 'Anil Ramakrishna, <b>Yixin Wan</b>, Kai-Wei Chang, Zhiqi Bu, Bhanukiran Vinzamuri, Volkan Cevher, Mingyi Hong, Rahul Gupta'
date: 06/2025
venue: 'EMNLP'
excerpt: ''
presentationurl: ''
paperurl: 'https://arxiv.org/abs/2502.15097'
topic: 'safety'
selected: 'true'
permalink: /publication/2025-lume
---
---
<a href='https://arxiv.org/abs/2502.15097.pdf' target="_blank">[Download Paper]</a>

<p align="justify">
Unlearning aims to remove copyrighted, sensitive, or private content from large language models (LLMs) without a full retraining. In this work, we develop a multi-task unlearning benchmark (LUME) which features three tasks: (1) unlearn synthetically generated creative short novels, (2) unlearn synthetic biographies with sensitive information, and (3) unlearn a collection of public biographies. We further release two fine-tuned LLMs of 1B and 7B parameter sizes as the target models. We conduct detailed evaluations of several recently proposed unlearning algorithms and present results on carefully crafted metrics to understand their behavior and limitations.
</p>